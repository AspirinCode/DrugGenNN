{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "from utils import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "matplotlib.rc('figure', figsize=(10, 5))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('chembl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~pd.isnull(df.acd_logd) & ~pd.isnull(df.acd_logp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['affinity'] = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(text):\n",
    "    \n",
    "    temp = [elems.index(start_elem)] + [elems.index(x) for x in text]\n",
    "    temp.append(elems.index(end_elem))\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smiles = df.canonical_smiles.apply(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_cell_size = 64\n",
    "num_layers = 1\n",
    "embedding_size = 32\n",
    "\n",
    "split_size = 30\n",
    "batchsize = 100\n",
    "epochs = 20\n",
    "stride=1\n",
    "\n",
    "num_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, sess, input_dim, output_dim, embedding_size, rnn_cell_size, num_layers, end_elem, sess_chkpt=''):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.rnn_cell_size = rnn_cell_size\n",
    "        self.num_layers = num_layers\n",
    "        self.state_shape =  num_layers*2*rnn_cell_size\n",
    "        self.end_elem = end_elem\n",
    "        \n",
    "        # Main architecrure\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float64, shape=[None, None, input_dim])\n",
    "        \n",
    "        self.state = tf.placeholder(tf.float64, shape=[None, self.state_shape])\n",
    "        \n",
    "        self.lengths = tf.placeholder(tf.float64, shape=[None])\n",
    "        \n",
    "        # Embedding\n",
    "        n_steps = tf.shape(self.X)[1]\n",
    "        \n",
    "        flat_X = tf.reshape(self.X, [-1, input_dim])\n",
    "        \n",
    "        embedding_matrix = tf.get_variable('Embedding_Matrix', shape=[input_dim, embedding_size], \n",
    "                                               initializer=xavier_initializer(), dtype=tf.float64)\n",
    "        \n",
    "        embedded = tf.reshape(tf.nn.relu(tf.matmul(flat_X, embedding_matrix)), [-1, n_steps, embedding_size])\n",
    "        \n",
    "        # RNN\n",
    "        rnn_cells = [tf.contrib.rnn.LSTMCell(rnn_cell_size, state_is_tuple=False, initializer=xavier_initializer()) \n",
    "                                 for i in range(num_layers)]\n",
    "\n",
    "        multiple_cells = tf.contrib.rnn.MultiRNNCell(rnn_cells, state_is_tuple=False)\n",
    "\n",
    "        rnn_output, self.new_state = tf.nn.dynamic_rnn(multiple_cells, embedded, initial_state=self.state, \n",
    "                                                       dtype=tf.float64, sequence_length=self.lengths)\n",
    "        \n",
    "        # First dense\n",
    "        \n",
    "        f_d = tf.layers.dense(rnn_output, rnn_cell_size//2, activation=tf.nn.relu, kernel_initializer=xavier_initializer())\n",
    "        \n",
    "        # Output\n",
    "        \n",
    "        output_logits = tf.layers.dense(f_d, output_dim, kernel_initializer=xavier_initializer())\n",
    "\n",
    "        self.output = tf.nn.softmax(output_logits)\n",
    "        \n",
    "        # Training utils\n",
    "        \n",
    "        self.Y = tf.placeholder(tf.float64, shape=[None, None, output_dim])\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_logits, labels=self.Y))\n",
    "        \n",
    "        # Optimizer \n",
    "        \n",
    "        self.optim = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        # Saver\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # Stuff\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        if len(sess_chkpt):\n",
    "            self.saver.restore(self.sess, sess_chkpt)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(self, X, Y, lengths):\n",
    "        \n",
    "        initial_states = np.zeros([len(X), self.state_shape])\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.X: X,\n",
    "            self.Y: Y,\n",
    "            self.state: initial_states,\n",
    "            self.lengths: lengths\n",
    "        }\n",
    "        \n",
    "        _, loss = self.sess.run([self.optim, self.loss], feed_dict=feed_dict)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def generate_element(self, x, zero_state=True):\n",
    "        \n",
    "        if zero_state:\n",
    "            state = np.zeros([1, self.state_shape])\n",
    "        else:\n",
    "            state = self.previous_state\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.X: [x],\n",
    "            self.state: state,\n",
    "            self.lengths: [len(x)]\n",
    "        }\n",
    "        \n",
    "        next_element, new_state = self.sess.run([self.output, self.new_state], feed_dict=feed_dict)\n",
    "        \n",
    "        self.previous_state = new_state\n",
    "        \n",
    "        return next_element\n",
    "    \n",
    "    def make_choice(self, probs):\n",
    "        return np.random.choice(np.arange(self.output_dim), p=probs)\n",
    "    \n",
    "    def generate_sequence(self, start, max_len=np.inf):\n",
    "        \n",
    "        num_features = self.input_dim - self.output_dim\n",
    "        \n",
    "        features = start[0][:num_features]\n",
    "        \n",
    "        sequence = [np.argmax(x[num_features:]) for x in start]\n",
    "\n",
    "        probs = self.generate_element(start, zero_state=True)\n",
    "        \n",
    "        next_element = self.make_choice(probs[0][0])\n",
    "\n",
    "        while next_element != self.end_elem and len(sequence) < max_len:\n",
    "        \n",
    "            sequence.append(next_element)\n",
    "            \n",
    "            next_input = np.concatenate([features, to_categorical(next_element, self.output_dim)[0]])\n",
    "            \n",
    "            probs = self.generate_element([next_input], False)\n",
    "            \n",
    "            next_element = self.make_choice(probs[0][0])\n",
    "            \n",
    "        return sequence[1:]\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.saver.save(self.sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x7fd5d7e40748>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(sess, len(elems)+num_features, len(elems), embedding_size, rnn_cell_size, num_layers, len(elems)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = df[[\"acd_logd\", \"acd_logp\", \"affinity\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_seq(seq, features):\n",
    "    \n",
    "    input_raw = to_categorical(seq[:-1], len(elems))\n",
    "    \n",
    "    input = np.array([np.concatenate([features, x]) for x in input_raw])\n",
    "    \n",
    "    outputs = to_categorical(seq[1:], len(elems))\n",
    "    \n",
    "    return input, outputs\n",
    "\n",
    "def iterate_minibatches(seqs, features, batchsize, shuffle=False):\n",
    "    \n",
    "    assert len(seqs) == len(features)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(seqs))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    for start_idx in range(0, len(seqs) - batchsize + 1, batchsize):\n",
    "        \n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        \n",
    "        lengths = list(map(lambda x: len(x)-1, seqs[excerpt]))\n",
    "        \n",
    "        our_seqs = pad_sequences(seqs[excerpt], padding='post')\n",
    "        \n",
    "        X, Y = zip(*map(lambda x: prepare_seq(x[0], x[1]), zip(*[our_seqs, features[excerpt]])))\n",
    "        \n",
    "        yield np.array(X), np.array(Y), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b48cad08148b69806aed253a743c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:  1op3c7[()HcO2cCcc2c[C7O)C))cc=c(4CeC[)cCCC)N=@=C))=]cC\n",
      "Sample:  SB1Cc5pcc)@cNO)H3((c2)C1c=1cC(pccNC)cnN<Start>C<Start>21OOc[)c2)=)2c)cc(N3cC5)Cg)OcOC422(C\n",
      "Sample:  1c)@))[COC)=C)cO)c@)CNCC3(CSC()c)=c%@(ccNC)\n"
     ]
    }
   ],
   "source": [
    "averages = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    \n",
    "    print(\"Epoch number: \", epoch)\n",
    "    \n",
    "    for X_batch, Y_batch, lengths in tqdm_notebook(iterate_minibatches(smiles.values, features, batchsize, True), \n",
    "                                             total=len(smiles)//batchsize):\n",
    "        \n",
    "        loss = model.train_step(X_batch, Y_batch, lengths)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if len(losses) % 50 == 0:\n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=[15, 10])\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "        \n",
    "        if len(losses) % 10 == 0:\n",
    "            \n",
    "            feat = features[np.random.choice(np.arange(0, features.shape[0]))]\n",
    "            \n",
    "            start_vec = to_categorical(elems.index(start_elem), len(elems))[0]\n",
    "            print(\"Sample: \", to_smile(model.generate_sequence([np.concatenate([feat, start_vec], 100)])))\n",
    "            \n",
    "            model.save('tmp/{}_{}.chkpt'.format(epoch, len(losses)))\n",
    "            \n",
    "    print(\"Epoch loss: \", np.mean(losses))\n",
    "    \n",
    "    averages.append(np.mean(losses))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
